{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b05af3-08b8-412e-bd58-01596d7f986b",
   "metadata": {},
   "source": [
    "# **Updater** for previously downloaded NASDAQ data using Yahoo! Finance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b189c-8312-4262-b695-67a7fa80f61d",
   "metadata": {},
   "source": [
    "Automate the process of downloading and updating historical data for all traded symbols on NASDAQ using the yFinance library. The script:\n",
    "\n",
    "- Retrieves the latest list of traded symbols from NASDAQ.\n",
    "- Checks for existing data files and updates them by downloading new data starting from an overlapping window (e.g., last 5 days) to ensure consistency.\n",
    "- Classifies symbols into subfolders (ETF, Stock, or Unknown) based on their ticker information.\n",
    "- Uses parallel processing to speed up the data retrieval process.\n",
    "- Automatically commits and pushes changes to a designated GitHub repository.\n",
    "\n",
    "## Strategy\n",
    "\n",
    "1. **Fetching Latest NASDAQ Traded Symbols**  \n",
    "   Download the latest symbol list from the NASDAQ Trader website (`nasdaqtraded.txt`). It filters out test issues and saves the full reference list as a CSV file in the data folder.\n",
    "\n",
    "2. **Data Download and Update**  \n",
    "   For each symbol:\n",
    "   - If a CSV file already exists:\n",
    "     - Read the existing file and determine the last recorded date.\n",
    "     - Download new data starting from a few days (overlap period) before the last date.\n",
    "     - Replace the overlapping data with the newly downloaded data and append any additional rows.\n",
    "   - If no file exists, download the full historical dataset.\n",
    "  \n",
    "3. **Classification**  \n",
    "   Each symbol is classified as an ETF, Stock, or Unknown by querying yFinance. The data is then stored in corresponding subdirectories.\n",
    "\n",
    "4. **Parallel Processing with Progress Reporting**  \n",
    "   The script leverages `ThreadPoolExecutor` for concurrent downloads. It prints and logs a progress update every 25 symbols, showing:\n",
    "   - The number of symbols processed.\n",
    "   - The number remaining.\n",
    "   - An estimated time remaining.\n",
    "\n",
    "5. **Git Integration**  \n",
    "   After all symbols are processed, the script uses Git commands to commit and push any changes made in the data folder to a specified GitHub repository.\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- **Python Version:** 3.6 or higher  \n",
    "- **Required Packages:** `pandas`, `yfinance`, `requests`  \n",
    "- **Git:** Must be installed and configured (the script must run inside a local clone of your GitHub repository)\n",
    "\n",
    "### Customization\n",
    "\n",
    "- **Overlap Days:**  \n",
    "  Adjust the `overlap_days` parameter (default is 5) in the `process_symbol` function call to change the overlap period used for refreshing data.\n",
    "\n",
    "- **Worker Threads:**  \n",
    "  Modify the `max_workers` parameter to optimize parallel processing for your environment.\n",
    "\n",
    "- **Git Commit Message:**  \n",
    "  Update the commit message in the `commit_and_push_changes` function as needed.\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "This script uses data provided by Yahoo Finance via the yFinance library. Data accuracy and timeliness are not guaranteed. Use the script at your own risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf52862-c9a3-46c4-b0c1-292f8dbdca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase yahoo finance is not installed...\n",
    "# !pip install --upgrade --no-cache-dir yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a454f9-6076-4fa4-9b2b-8270fb5ffbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from io import StringIO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d09625-26b6-4eb8-aeb6-d1593a85f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Global session setup for better connection pooling\n",
    "# ---------------------------\n",
    "global_session = requests.Session()\n",
    "adapter = HTTPAdapter(pool_connections=100, pool_maxsize=100)\n",
    "global_session.mount(\"http://\", adapter)\n",
    "global_session.mount(\"https://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caedbda-4e51-49ee-9668-8b20520f0ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable InsecureRequestWarning (only for testing purposes)\n",
    "requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Configure logging to log errors to \"errors.log\"\n",
    "logging.basicConfig(filename=\"errors.log\", level=logging.ERROR,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09df69-7717-443b-9184-b874a70cfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Get the latest list of traded symbols from NASDAQ\n",
    "# ---------------------------\n",
    "def get_nasdaq_symbols(data_dir):\n",
    "    \"\"\"\n",
    "    Download the list of NASDAQ traded symbols from the NASDAQ Trader website.\n",
    "    The file is pipe-separated and contains footer rows and test issues.\n",
    "    Filters out rows where 'Test Issue' is not 'N', saves the full reference\n",
    "    DataFrame to the provided directory, and returns the list of symbols.\n",
    "    \"\"\"\n",
    "    url = \"http://www.nasdaqtrader.com/dynamic/SymDir/nasdaqtraded.txt\"\n",
    "    try:\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()\n",
    "        data = StringIO(response.text)\n",
    "        df = pd.read_csv(data, sep=\"|\")\n",
    "        # Filter out rows where 'Test Issue' is not 'N'\n",
    "        df_clean = df[df['Test Issue'] == 'N']\n",
    "        df_clean = df_clean[df_clean['Symbol'].notna()]\n",
    "        # Persist the reference list to a CSV file\n",
    "        ref_file = os.path.join(data_dir, \"nasdaq_symbols_reference.csv\")\n",
    "        df_clean.to_csv(ref_file, index=False)\n",
    "        symbols = df_clean['Symbol'].tolist()\n",
    "        return symbols\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching NASDAQ traded symbols: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961b50e-928f-4ab6-a3bd-d78e6215942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Classification helper (ETF/Stock)\n",
    "# ---------------------------\n",
    "def classify_symbol(symbol):\n",
    "    \"\"\"\n",
    "    Classify the symbol as 'ETF' or 'Stock' using yfinance Ticker info.\n",
    "    Returns \"Unknown\" if classification is unavailable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol, session=global_session)\n",
    "        info = ticker.info\n",
    "        qtype = info.get(\"quoteType\", None)\n",
    "        if qtype == \"ETF\":\n",
    "            return \"ETF\"\n",
    "        elif qtype == \"EQUITY\":\n",
    "            return \"Stock\"\n",
    "        else:\n",
    "            logging.error(f\"Unknown or missing quoteType for {symbol}. Info: {info}\")\n",
    "            return \"Unknown\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error classifying symbol {symbol}: {e}\")\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef16d81-d0be-4917-bcc9-3557d1f906a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3. Process a single symbol: update or download full data\n",
    "# ---------------------------\n",
    "def process_symbol(symbol, folders, overlap_days=5):\n",
    "    \"\"\"\n",
    "    For the given symbol:\n",
    "    - Classify and determine the correct folder.\n",
    "    - If a CSV file already exists:\n",
    "         * Read it with an inferred datetime format.\n",
    "         * Determine the last date.\n",
    "         * Download new data starting from (last_date - overlap_days).\n",
    "         * Replace the overlapping portion with new data and append new rows.\n",
    "    - If no file exists, download the full historical data.\n",
    "    Returns a status message.\n",
    "    \"\"\"\n",
    "    classification = classify_symbol(symbol)\n",
    "    if classification not in folders:\n",
    "        classification = \"Unknown\"\n",
    "    file_path = os.path.join(folders[classification], f\"{symbol}.csv\")\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            # File exists: update existing data.\n",
    "            old_df = pd.read_csv(file_path, index_col=0, parse_dates=True, infer_datetime_format=True)\n",
    "            if old_df.empty:\n",
    "                raise ValueError(\"Existing file is empty.\")\n",
    "            last_date = old_df.index.max()\n",
    "            # Determine overlap start date as (last_date - overlap_days)\n",
    "            overlap_start = last_date - pd.Timedelta(days=overlap_days)\n",
    "            # Download new data starting from overlap_start date.\n",
    "            new_df = yf.download(symbol,\n",
    "                                 start=overlap_start.strftime(\"%Y-%m-%d\"),\n",
    "                                 progress=False,\n",
    "                                 session=global_session,\n",
    "                                 auto_adjust=True)\n",
    "            if new_df is None or new_df.empty:\n",
    "                return f\"No new data for {symbol}\"\n",
    "            # Exclude overlapping data from the old dataset.\n",
    "            updated_df = pd.concat([old_df[old_df.index < overlap_start], new_df])\n",
    "            updated_df.sort_index(inplace=True)\n",
    "            updated_df.to_csv(file_path)\n",
    "            return f\"Updated data for {symbol} in folder '{classification}'\"\n",
    "        else:\n",
    "            # File does not exist: download full historical data.\n",
    "            data = yf.download(symbol,\n",
    "                               period=\"max\",\n",
    "                               progress=False,\n",
    "                               session=global_session,\n",
    "                               auto_adjust=True)\n",
    "            if data is None or data.empty:\n",
    "                logging.error(f\"No data available for {symbol}\")\n",
    "                return f\"No data available for {symbol}\"\n",
    "            data.to_csv(file_path)\n",
    "            return f\"Downloaded and saved data for {symbol} in folder '{classification}'\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for {symbol}: {e}\")\n",
    "        return f\"Error processing data for {symbol}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bdd57-3b60-49e3-847f-1184457d9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Git commit and push changes\n",
    "# ---------------------------\n",
    "def commit_and_push_changes(repo_path, commit_message=\"Update data\"):\n",
    "    \"\"\"\n",
    "    Add changes in the 'data' folder, commit them with a provided message,\n",
    "    and push the commit to the remote repository.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"add\", \"data\"], cwd=repo_path, check=True)\n",
    "        full_message = f\"{commit_message} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", full_message], cwd=repo_path, check=True)\n",
    "        subprocess.run([\"git\", \"push\"], cwd=repo_path, check=True)\n",
    "        print(\"Changes have been committed and pushed to the repository.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"Git operation failed: {e}\")\n",
    "        print(\"Failed to commit and push changes. See errors.log for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b49e05-527c-4dc7-a82f-98033d17d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Main routine\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # Create main data directory and subfolders for classifications.\n",
    "    base_dir = \"data\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    folders = {\n",
    "        \"ETF\": os.path.join(base_dir, \"ETF\"),\n",
    "        \"Stock\": os.path.join(base_dir, \"Stock\"),\n",
    "        \"Unknown\": os.path.join(base_dir, \"Unknown\")\n",
    "    }\n",
    "    for folder in folders.values():\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    # Fetch and persist the latest NASDAQ traded symbols.\n",
    "    symbols = get_nasdaq_symbols(base_dir)\n",
    "    if not symbols:\n",
    "        print(\"No symbols found. Please check errors.log for details.\")\n",
    "        return\n",
    "    total_symbols = len(symbols)\n",
    "    print(f\"Found {total_symbols} traded symbols on NASDAQ. Reference list saved to {os.path.join(base_dir, 'nasdaq_symbols_reference.csv')}\")\n",
    "    \n",
    "    # Process each symbol in parallel with progress indicator.\n",
    "    max_workers = 25  # Adjust as needed.\n",
    "    processed_count = 0\n",
    "    start_time = time.time()\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_symbol = {executor.submit(process_symbol, sym, folders, 5): sym for sym in symbols}\n",
    "        \n",
    "        for future in as_completed(future_to_symbol):\n",
    "            symbol = future_to_symbol[future]\n",
    "            processed_count += 1\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results[symbol] = result\n",
    "                print(result)\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"{symbol} generated an exception: {exc}\")\n",
    "                print(f\"{symbol} generated an exception. See errors.log for details.\")\n",
    "            \n",
    "            # Every 25 symbols, log progress.\n",
    "            if processed_count % 25 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                avg_time = elapsed / processed_count\n",
    "                remaining = total_symbols - processed_count\n",
    "                est_seconds = avg_time * remaining\n",
    "                est_hours = est_seconds / 3600\n",
    "                progress_msg = (f\"Processed {processed_count} out of {total_symbols} symbols, \"\n",
    "                                f\"{remaining} remaining, estimated time left: {est_hours:.2f} hours or {est_hours*60:.2f} minutes\")\n",
    "                logging.info(progress_msg)\n",
    "                print(progress_msg)\n",
    "    \n",
    "    # After processing all symbols, commit and push changes.\n",
    "    commit_and_push_changes(repo_path=\".\", commit_message=\"Update NASDAQ data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87ec16-b5e9-41ea-abab-787e1ecf8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183de18e-7b02-408b-b29f-7844f11c22fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3392ad-bc5b-446c-be9c-421409c2a62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed4cfe-aebb-47ba-8655-6794ef675057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3070198-e532-4cf5-8325-142a8e4f2662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e26382-753c-4801-820c-5a9bd2fe27c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ce88b-9228-4f15-bc3e-39673989cd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4532cb-dbf7-4961-9e3a-4da758191d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d485e1-458f-427e-ab93-bfb7e4382cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50096b-a677-47d2-8509-543d8c22ad39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143708a-21bd-43ef-85ab-ccec59403c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40a40c-b3b2-49fb-813c-72002276e008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a31c83-9b7b-457e-ad86-1efd3ee74acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfff4b8-61d3-47e6-b5f1-9894ce16cbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562ec48-9a67-48ad-8350-88eefeec6921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a178e-783b-40b8-b2e7-eca5c74fe483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
